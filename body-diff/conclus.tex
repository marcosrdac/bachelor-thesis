%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                         CONCLUSOES                              %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusões}

  Neste material foram introduzidos os conceitos básicos de aprendizado estatístico, seus métodos de otimização e a extensão de seus modelos mais simples às redes neurais. A extensão da rede neural recorrente de Elman para um modelo capaz de realizar simulações acústicas foi teorizada e implementada, gerando resultados satisfatórios de modelagem e inversão utilizando três otimizadores típicos do universo das redes neurais: SGD, Momento e Adam. Problemas de otimização multi-frequência devem ser corrigidos em trabalhos futuros por meio da implementação de técnicas de agendamento no processo de otimização, as quais são tidas como tecnologias relativamente avançadas de treinamento de redes. Uma pequena inadequação próxima à superfície dos modelos invertidos foi encontrada, a qual é uma suposta consequência do uso de poucos tiros para a inversão, mas que deve ser alvo de futuras investigações. É necessário salientar que, apesar da possibilidade de realização da FWI pelo método aqui apresentado, a demanda por memória no cálculo dos gradientes se torna superior à comumente utilizada em casos de aplicação real. Em casos similares, a solução do método adjunto pode permitir a implementação de otimizações valorosas.

  Do ponto de vista da modelagem tradicional, a generalidade do método empregado abre janelas à pesquisa neste e em outros problemas físicos, como a dispersão de poluentes em fluidos, a dissipação de calor ou a modelagem de deformações numa estrutura mecânica, uma vez que a inversão aqui realizada acontece por diferenciação automática, sem mesmo o uso explícito do método adjunto. Este ponto é muito valoroso, pois o método adjunto exige uma etapa de pré-formulação de sua teoria com relação ao problema estudado da qual sua implementação depende, enquanto que a inversão por diferenciação automática é de implementação imediata para qualquer problema que pode ser expresso num grafo de computações comuns com diferenciais conhecidas. Uma contribuição interessante vinda do contexto das redes neurais é a de interrupção precoce, técnica que permitiu uma inversão controlada na FWI aqui implementada. Desta forma, ao invés de um número fixo de iterações de otimização, um critério de parada baseado na modelagem sobre tiros de validação do campo de velocidades otimizado foi implementado.

  Do ponto de vista das redes neurais, este trabalho \DIFdelbegin \DIFdel{constroi }\DIFdelend \DIFaddbegin \DIFadd{constrói }\DIFaddend uma rede com bons resultados e parâmetros facilmente interpretáveis --- as velocidades em subsuperfície. A junção da área das redes com a da inversão acústica traz uma ideia não muito comum ao treinamento das redes: é possível que um treinamento multiescala seja útil para também para o treinamento de outras redes neurais. Esta ideia é diretamente aplicável a redes de segmentação semântica, por exemplo. Vale citarmos que a metodologia das redes neurais informadas de física --- isto é, a penalização de insatisfações de equações físicas dentro da função custo utilizada na inversão --- é um próximo passo a ser introduzido ao modelo físico desenvolvido neste material e tratado em trabalhos futuros.

  Creio que este material é adequado à leitura por estudantes da Geofísica que desejem ser introduzidos ao universo do aprendizado de máquina, das redes neurais ou à diferenciação automática, ou ao mesmo, aos que desejem revisar seus tópicos básicos, os quais foram essenciais à implementação alvo deste trabalho.
